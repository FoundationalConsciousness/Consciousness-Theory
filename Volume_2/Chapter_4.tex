\chapter{Информация, энтропия и различие}

\section{Различие как фундаментальная величина}

В Томе~I было показано,
что различие является первоосновой возникновения форм,
пространства, времени и наблюдения.

Цель данной главы —
ввести строгую математическую формализацию различия
через понятия информации и энтропии.

Важно подчеркнуть:
информация здесь не является сообщением или данными,
а представляет собой количественную меру различия
между возможными состояниями Сознания.

\section{Вероятностное описание состояний}

Пусть система может находиться
в одном из состояний $\{ s_i \}$ с вероятностями $\{ p_i \}$,
где:
\[
p_i \geq 0, \quad \sum_i p_i = 1.
\]

Вероятностное распределение $\{ p_i \}$
описывает степень актуализации форм знания
в данной локализации наблюдателя.

\section{Информация как мера различия}

Информация возникает тогда,
когда возможные состояния различимы.

Количество информации,
связанное с состоянием $s_i$,
определяется как:
\[
I(s_i) = - \log p_i.
\]

Редкие состояния несут больше информации,
поскольку они представляют более выраженное различие
относительно типичных конфигураций знания.

\section{Энтропия Шеннона}

Средняя информация по всем возможным состояниям
определяется энтропией Шеннона:
\[
H = - \sum_i p_i \log p_i.
\]

Энтропия измеряет степень неопределённости
или, эквивалентно,
среднюю степень различия в системе состояний.

В онтологическом смысле
энтропия характеризует богатство структуры различий.

\section{Минимальная и максимальная энтропия}

Минимальная энтропия достигается,
когда одно состояние имеет вероятность $1$:
\[
H_{\min} = 0.
\]

Это соответствует полной стабилизации формы знания.

Максимальная энтропия достигается
при равномерном распределении:
\[
p_i = \frac{1}{N}, \quad
H_{\max} = \log N.
\]

Это состояние максимальной неопределённости
и максимального потенциала различий.

\section{Энтропия и динамика}

В процессе динамики состояний
энтропия может изменяться.

Устойчивые структуры характеризуются
относительно постоянной или медленно изменяющейся энтропией.

Рост энтропии отражает расширение пространства возможных различий,
в то время как её уменьшение связано
с процессами локализации и наблюдения.

\section{Условная энтропия и знание наблюдателя}

Пусть наблюдатель располагает дополнительным знанием $Y$.
Тогда условная энтропия определяется как:
\[
H(X|Y) = H(X,Y) - H(Y).
\]

Уменьшение условной энтропии
соответствует уточнению структуры различий
в результате наблюдения.

Это формально выражает
процесс локализации знания,
описанный в Томе~I.

\section{Информация и причинность}

Если знание о состоянии $X$ уменьшает неопределённость $Y$,
то между ними возникает причинная связь
в статистическом смысле.

Таким образом,
причинность может быть формализована
через информационные зависимости,
а не через жёсткую детерминацию.

\section{Онтологическая интерпретация}

В рамках Теории Сознания:
\begin{itemize}
    \item информация — мера различия;
    \item энтропия — мера богатства возможных форм;
    \item уменьшение энтропии — акт локализации знания;
    \item рост энтропии — расширение потенциала различий.
\end{itemize}

Формализм информации не заменяет онтологию,
а делает её количественно выразимой.

\section{Выводы}

В данной главе было показано, что:
\begin{itemize}
    \item различие может быть количественно выражено через информацию;
    \item энтропия является фундаментальной характеристикой структуры состояний;
    \item наблюдение связано с уменьшением условной энтропии;
    \item причинность имеет информационную природу.
\end{itemize}

Это создаёт основу
для введения метрик различия
и геометрии пространства состояний,
которые будут рассмотрены далее.
